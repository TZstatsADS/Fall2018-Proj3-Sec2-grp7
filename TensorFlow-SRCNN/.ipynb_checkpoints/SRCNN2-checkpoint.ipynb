{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### utils.py, version 2 ###\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os \n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "# Get the Image\n",
    "def imread(path):\n",
    "    img = cv2.imread(path)\n",
    "    return img\n",
    "\n",
    "def imsave(image, path, config):\n",
    "    #checkimage(image)\n",
    "    # Check the check dir, if not, create one\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(),config.result_dir)):\n",
    "        os.makedirs(os.path.join(os.getcwd(),config.result_dir))\n",
    "\n",
    "    # NOTE: because normial, we need mutlify 255 back    \n",
    "    cv2.imwrite(os.path.join(os.getcwd(),path),image * 255.)\n",
    "\n",
    "def checkimage(image):\n",
    "    cv2.imshow(\"test\",image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "def modcrop(img, scale =3):\n",
    "    \"\"\"\n",
    "        To scale down and up the original image, first thing to do is to have no remainder while scaling operation.\n",
    "    \"\"\"\n",
    "    # Check the image is grayscale\n",
    "    if len(img.shape) ==3:\n",
    "        h, w, _ = img.shape\n",
    "        h = (h // scale) * scale\n",
    "        w = (w // scale) * scale\n",
    "        img = img[0:h, 0:w, :]\n",
    "    else:\n",
    "        h, w = img.shape\n",
    "        h = (h // scale) * scale\n",
    "        w = (w // scale) * scale\n",
    "        img = img[0:h, 0:w]\n",
    "    return img\n",
    "\n",
    "def checkpoint_dir(config):\n",
    "    if config.is_train:\n",
    "        return os.path.join('./{}'.format(config.checkpoint_dir), \"train.h5\")\n",
    "    else:\n",
    "        return os.path.join('./{}'.format(config.checkpoint_dir), \"test.h5\")\n",
    "\n",
    "def preprocess(path ,scale = 3):\n",
    "    img = imread(path)\n",
    "\n",
    "    label_ = modcrop(img, scale)\n",
    "    \n",
    "    bicbuic_img = cv2.resize(label_,None,fx = 1.0/scale ,fy = 1.0/scale, interpolation = cv2.INTER_CUBIC)# Resize by scaling factor\n",
    "    input_ = cv2.resize(bicbuic_img,None,fx = scale ,fy=scale, interpolation = cv2.INTER_CUBIC)# Resize by scaling factor\n",
    "    return input_, label_\n",
    "\n",
    "def prepare_data(dataset=\"Train\",Input_img=\"\"):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            dataset: choose train dataset or test dataset\n",
    "            For train dataset, output data would be ['.../t1.bmp', '.../t2.bmp',..., 't99.bmp']\n",
    "    \"\"\"\n",
    "    if dataset == \"Train\":\n",
    "        data_dir = os.path.join(os.getcwd(), dataset) # Join the Train dir to current directory\n",
    "        data = glob.glob(os.path.join(data_dir, \"*.bmp\")) # make set of all dataset file path\n",
    "    else:\n",
    "        if Input_img !=\"\":\n",
    "            data = [os.path.join(os.getcwd(),Input_img)]\n",
    "        else:\n",
    "            data_dir = os.path.join(os.path.join(os.getcwd(), dataset), \"Set5\")\n",
    "            data = glob.glob(os.path.join(data_dir, \"*.bmp\")) # make set of all dataset file path\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "def load_data(is_train, test_img):\n",
    "    if is_train:\n",
    "        data = prepare_data(dataset=\"Train\")\n",
    "    else:\n",
    "        if test_img != \"\":\n",
    "            return prepare_data(dataset=\"Test\",Input_img=test_img)\n",
    "        data = prepare_data(dataset=\"Test\")\n",
    "    return data\n",
    "\n",
    "def make_sub_data(data, padding, config):\n",
    "    \"\"\"\n",
    "        Make the sub_data set\n",
    "        Args:\n",
    "            data : the set of all file path \n",
    "            padding : the image padding of input to label\n",
    "            config : the all flags\n",
    "    \"\"\"\n",
    "    sub_input_sequence = []\n",
    "    sub_label_sequence = []\n",
    "    for i in range(len(data)):\n",
    "        if config.is_train:\n",
    "            input_, label_, = preprocess(data[i], config.scale) # do bicbuic\n",
    "        else: # Test just one picture\n",
    "            input_, label_, = preprocess(data[i], config.scale) # do bicbuic\n",
    "        \n",
    "        if len(input_.shape) == 3: # is color\n",
    "            h, w, c = input_.shape\n",
    "        else:\n",
    "            h, w = input_.shape # is grayscale\n",
    "        #checkimage(input_)\n",
    "        nx, ny = 0, 0\n",
    "        for x in range(0, h - config.image_size + 1, config.stride):\n",
    "            nx += 1; ny = 0\n",
    "            for y in range(0, w - config.image_size + 1, config.stride):\n",
    "                ny += 1\n",
    "\n",
    "                sub_input = input_[x: x + config.image_size, y: y + config.image_size] # 33 * 33\n",
    "                sub_label = label_[x + padding: x + padding + config.label_size, y + padding: y + padding + config.label_size] # 21 * 21\n",
    "\n",
    "\n",
    "                # Reshape the subinput and sublabel\n",
    "                sub_input = sub_input.reshape([config.image_size, config.image_size, config.c_dim])\n",
    "                sub_label = sub_label.reshape([config.label_size, config.label_size, config.c_dim])\n",
    "                # Normialize\n",
    "                sub_input =  sub_input / 255.0\n",
    "                sub_label =  sub_label / 255.0\n",
    "                \n",
    "                #cv2.imshow(\"im1\",sub_input)\n",
    "                #cv2.imshow(\"im2\",sub_label)\n",
    "                #cv2.waitKey(0)\n",
    "\n",
    "                # Add to sequence\n",
    "                sub_input_sequence.append(sub_input)\n",
    "                sub_label_sequence.append(sub_label)\n",
    "\n",
    "        \n",
    "    # NOTE: The nx, ny can be ignore in train\n",
    "    return sub_input_sequence, sub_label_sequence, nx, ny\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    \"\"\"\n",
    "        Read h5 format data file\n",
    "\n",
    "        Args:\n",
    "            path: file path of desired file\n",
    "            data: '.h5' file format that contains  input values\n",
    "            label: '.h5' file format that contains label values \n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as hf:\n",
    "        input_ = np.array(hf.get('input'))\n",
    "        label_ = np.array(hf.get('label'))\n",
    "        return input_, label_\n",
    "\n",
    "def make_data_hf(input_, label_, config):\n",
    "    \"\"\"\n",
    "        Make input data as h5 file format\n",
    "        Depending on \"is_train\" (flag value), savepath would be change.\n",
    "    \"\"\"\n",
    "    # Check the check dir, if not, create one\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(),config.checkpoint_dir)):\n",
    "        os.makedirs(os.path.join(os.getcwd(),config.checkpoint_dir))\n",
    "\n",
    "    if config.is_train:\n",
    "        savepath = os.path.join(os.getcwd(), config.checkpoint_dir + '/train.h5')\n",
    "    else:\n",
    "        savepath = os.path.join(os.getcwd(), config.checkpoint_dir + '/test.h5')\n",
    "\n",
    "    with h5py.File(savepath, 'w') as hf:\n",
    "        hf.create_dataset('input', data=input_)\n",
    "        hf.create_dataset('label', data=label_)\n",
    "\n",
    "def merge(images, size, c_dim):\n",
    "    \"\"\"\n",
    "        images is the sub image set, merge it\n",
    "    \"\"\"\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    \n",
    "    img = np.zeros((h*size[0], w*size[1], c_dim))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j * h : j * h + h,i * w : i * w + w, :] = image\n",
    "        #cv2.imshow(\"srimg\",img)\n",
    "        #cv2.waitKey(0)\n",
    "        \n",
    "    return img\n",
    "\n",
    "def input_setup(config):\n",
    "    \"\"\"\n",
    "        Read image files and make their sub-images and saved them as a h5 file format\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data path, if is_train False, get test data\n",
    "    data = load_data(config.is_train, config.test_img)\n",
    "\n",
    "    padding = abs(config.image_size - config.label_size) / 2 \n",
    "\n",
    "    # Make sub_input and sub_label, if is_train false more return nx, ny\n",
    "    sub_input_sequence, sub_label_sequence, nx, ny = make_sub_data(data, padding, config)\n",
    "\n",
    "\n",
    "    # Make list to numpy array. With this transform\n",
    "    arrinput = np.asarray(sub_input_sequence) # [?, 33, 33, 3]\n",
    "    arrlabel = np.asarray(sub_label_sequence) # [?, 21, 21, 3]\n",
    "\n",
    "    make_data_hf(arrinput, arrlabel, config)\n",
    "\n",
    "    return nx, ny\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### model.py ###\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils import (\n",
    "    input_setup,\n",
    "    checkpoint_dir,\n",
    "    read_data,\n",
    "    merge,\n",
    "    checkimage,\n",
    "    imsave\n",
    ")\n",
    "class SRCNN(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 sess,\n",
    "                 image_size,\n",
    "                 label_size,\n",
    "                 c_dim):\n",
    "        self.sess = sess\n",
    "        self.image_size = image_size\n",
    "        self.label_size = label_size\n",
    "        self.c_dim = c_dim\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.images = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, self.c_dim], name='images')\n",
    "        self.labels = tf.placeholder(tf.float32, [None, self.label_size, self.label_size, self.c_dim], name='labels')\n",
    "        \n",
    "        self.weights = {\n",
    "            'w1': tf.Variable(tf.random_normal([9, 9, self.c_dim, 64], stddev=1e-3), name='w1'),\n",
    "            'w2': tf.Variable(tf.random_normal([1, 1, 64, 32], stddev=1e-3), name='w2'),\n",
    "            'w3': tf.Variable(tf.random_normal([5, 5, 32, self.c_dim], stddev=1e-3), name='w3')\n",
    "        }\n",
    "\n",
    "        self.biases = {\n",
    "            'b1': tf.Variable(tf.zeros([64], name='b1')),\n",
    "            'b2': tf.Variable(tf.zeros([32], name='b2')),\n",
    "            'b3': tf.Variable(tf.zeros([self.c_dim], name='b3'))\n",
    "        }\n",
    "        \n",
    "        self.pred = self.model()\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.labels - self.pred))\n",
    "\n",
    "        self.saver = tf.train.Saver() # To save checkpoint\n",
    "\n",
    "    def model(self):\n",
    "        conv1 = tf.nn.relu(tf.nn.conv2d(self.images, self.weights['w1'], strides=[1,1,1,1], padding='VALID') + self.biases['b1'])\n",
    "        conv2 = tf.nn.relu(tf.nn.conv2d(conv1, self.weights['w2'], strides=[1,1,1,1], padding='VALID') + self.biases['b2'])\n",
    "        conv3 = tf.nn.conv2d(conv2, self.weights['w3'], strides=[1,1,1,1], padding='VALID') + self.biases['b3'] # This layer don't need ReLU\n",
    "        return conv3\n",
    "\n",
    "    def train(self, config):\n",
    "        \n",
    "        # NOTE : if train, the nx, ny are ingnored\n",
    "        nx, ny = input_setup(config)\n",
    "\n",
    "        data_dir = checkpoint_dir(config)\n",
    "        \n",
    "        input_, label_ = read_data(data_dir)\n",
    "        # Stochastic gradient descent with the standard backpropagation\n",
    "        #self.train_op = tf.train.GradientDescentOptimizer(config.learning_rate).minimize(self.loss)\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=config.learning_rate).minimize(self.loss)\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        counter = 0\n",
    "        time_ = time.time()\n",
    "\n",
    "        self.load(config.checkpoint_dir)\n",
    "        # Train\n",
    "        if config.is_train:\n",
    "            print(\"Now Start Training...\")\n",
    "            for ep in range(config.epoch):\n",
    "                # Run by batch images\n",
    "                batch_idxs = len(input_) // config.batch_size\n",
    "                for idx in range(0, batch_idxs):\n",
    "                    batch_images = input_[idx * config.batch_size : (idx + 1) * config.batch_size]\n",
    "                    batch_labels = label_[idx * config.batch_size : (idx + 1) * config.batch_size]\n",
    "                    counter += 1\n",
    "                    _, err = self.sess.run([self.train_op, self.loss], feed_dict={self.images: batch_images, self.labels: batch_labels})\n",
    "\n",
    "                    if counter % 10 == 0:\n",
    "                        print(\"Epoch: [%2d], step: [%2d], time: [%4.4f], loss: [%.8f]\" % ((ep+1), counter, time.time()-time_, err))\n",
    "                        #print(label_[1] - self.pred.eval({self.images: input_})[1],'loss:]',err)\n",
    "                    if counter % 500 == 0:\n",
    "                        self.save(config.checkpoint_dir, counter)\n",
    "        # Test\n",
    "        else:\n",
    "            print(\"Now Start Testing...\")\n",
    "            #print(\"nx\",\"ny\",nx,ny)\n",
    "            \n",
    "            result = self.pred.eval({self.images: input_})\n",
    "            #print(label_[1] - result[1])\n",
    "            image = merge(result, [nx, ny], self.c_dim)\n",
    "            #image_LR = merge(input_, [nx, ny], self.c_dim)\n",
    "            #checkimage(image_LR)\n",
    "            imsave(image, config.result_dir+'/result.png', config)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        \"\"\"\n",
    "            To load the checkpoint use to test or pretrain\n",
    "        \"\"\"\n",
    "        print(\"\\nReading Checkpoints.....\\n\\n\")\n",
    "        model_dir = \"%s_%s\" % (\"srcnn\", self.label_size)# give the model name by label_size\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        \n",
    "        # Check the checkpoint is exist \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_path = str(ckpt.model_checkpoint_path) # convert the unicode to string\n",
    "            self.saver.restore(self.sess, os.path.join(os.getcwd(), ckpt_path))\n",
    "            print(\"\\n Checkpoint Loading Success! %s\\n\\n\"% ckpt_path)\n",
    "        else:\n",
    "            print(\"\\n! Checkpoint Loading Failed \\n\\n\")\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        \"\"\"\n",
    "            To save the checkpoint use to test or pretrain\n",
    "        \"\"\"\n",
    "        model_name = \"SRCNN.model\"\n",
    "        model_dir = \"%s_%s\" % (\"srcnn\", self.label_size)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "             os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt26.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t53.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t47.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t46.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t52.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt27.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt19.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt25.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t44.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t50.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t51.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t45.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt24.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt18.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt20.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t55.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t54.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t40.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt21.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt23.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t56.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t42.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt8.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt9.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t43.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t57.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt22.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t18.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t30.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t24.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t25.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t31.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t19.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t27.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t33.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t32.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t26.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t9.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t22.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t36.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t37.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t23.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t8.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t35.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t21.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t20.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t34.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t6.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t39.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t11.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t10.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t38.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t7.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t5.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t12.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t13.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t4.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t17.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t16.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t1.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t3.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t14.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t28.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t29.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t15.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t2.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt13.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t66.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt4.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt5.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt12.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt10.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t59.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t65.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt7.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt6.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t64.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t58.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt15.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t60.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t48.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt2.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt3.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t49.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t61.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt14.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt16.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t63.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt1.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/t62.bmp', '/Users/yaoqi/Google Drive/GitHub/TensorFlow-SRCNN2/Train/tt17.bmp']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f88354839118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# parse the command argument , the call the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f88354839118>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     26\u001b[0m                       c_dim = FLAGS.c_dim)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/GitHub/TensorFlow-SRCNN2/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# NOTE : if train, the nx, ny are ingnored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/GitHub/TensorFlow-SRCNN2/utils.py\u001b[0m in \u001b[0;36minput_setup\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Make sub_input and sub_label, if is_train false more return nx, ny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0msub_input_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_label_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sub_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/GitHub/TensorFlow-SRCNN2/utils.py\u001b[0m in \u001b[0;36mmake_sub_data\u001b[0;34m(data, padding, config)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# do bicbuic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Test just one picture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# do bicbuic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/GitHub/TensorFlow-SRCNN2/utils.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(path, scale)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mlabel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbicbuic_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mfy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# Resize by scaling factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/GitHub/TensorFlow-SRCNN2/utils.py\u001b[0m in \u001b[0;36mmodcrop\u001b[0;34m(img, scale)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "### main.py ###\n",
    "import tensorflow as tf\n",
    "from model import SRCNN\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\"epoch\", 1500, \"Number of epoch\")\n",
    "flags.DEFINE_integer(\"image_size\", 33, \"The size of image input\")\n",
    "flags.DEFINE_integer(\"label_size\", 21, \"The size of image output\")\n",
    "flags.DEFINE_integer(\"c_dim\", 3, \"The size of channel\")\n",
    "flags.DEFINE_boolean(\"is_train\", True, \"if the train\")\n",
    "flags.DEFINE_integer(\"scale\", 3, \"the size of scale factor for preprocessing input image\")\n",
    "flags.DEFINE_integer(\"stride\", 21, \"the size of stride\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Name of checkpoint directory\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-4 , \"The learning rate\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"the size of batch\")\n",
    "flags.DEFINE_string(\"result_dir\", \"result\", \"Name of result directory\")\n",
    "flags.DEFINE_string(\"test_img\", \"\", \"test_img\")\n",
    "\n",
    "\n",
    "\n",
    "def main(_): #?\n",
    "    with tf.Session() as sess:\n",
    "        srcnn = SRCNN(sess,\n",
    "                      image_size = FLAGS.image_size,\n",
    "                      label_size = FLAGS.label_size,\n",
    "                      c_dim = FLAGS.c_dim)\n",
    "\n",
    "        srcnn.train(FLAGS)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    tf.app.run() # parse the command argument , the call the main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
